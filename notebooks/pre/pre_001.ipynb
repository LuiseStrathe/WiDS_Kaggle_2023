{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING --version 001\n",
    "- radical drop of colunms\n",
    "- date split into Y / W\n",
    "\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_ID = '001'\n",
    "version_note = '_coldrop_datesplit_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import warnings\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300587, 246) (300587, 246)\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "X = pd.read_csv('../../data/raw/train_data.csv.zip')\n",
    "summer_weeks = pd.read_csv('../../data/additional/summer_intensity.csv')\n",
    "\n",
    "#split dataset into train and test\n",
    "X_train_input, X_test_input = train_test_split(X, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(X_train_input.shape, X_train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper data\n",
    "summer_weeks = pd.read_csv('../../data/additional/summer_intensity.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare columns & NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get parameters for normalization --> avoid data leekage\n",
    "norm_paras = X_train_input.agg(['mean', 'std', 'median'])\n",
    "\n",
    "# reduce expanded features\n",
    "collapse = []\n",
    "coll_patt = ['.+vwnd-250', '.+uwnd-250', '.+vwnd-925', '.+uwnd-925', '.+hgt-850', '.+hgt-500', '.+hgt-10', '.+hgt-100', '^icec', '^sst']\n",
    "\n",
    "for p in coll_patt:\n",
    "    pt = re.compile(p)\n",
    "    collapse += [[c for c in X_train_input.columns if pt.match(c)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get drop columns\n",
    "drops = ['startdate']\n",
    "patterns = ['^nmme', '^wind', '^icec', '^sst']\n",
    "\n",
    "for p in patterns:\n",
    "    p = re.compile(p)\n",
    "    drops += [k for k in X_train_input.columns if bool(p.match(k))]\n",
    "#print(len(drops), '\\n', drops)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, norm_paras=norm_paras):\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    length = X.shape[0]\n",
    "    target = 'contest-tmp2m-14d__tmp2m'\n",
    "\n",
    "    # extract y\n",
    "    if target in X.columns.values:\n",
    "        y = X[target].to_numpy()\n",
    "        X = X.drop(labels=target, axis=1)\n",
    "    else:    y = None\n",
    "    \n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    # collapse expanded features\n",
    "    for i in range(len(coll_patt)):\n",
    "        name = coll_patt[i].strip('.+-')\n",
    "        X[f'{name}_mn'] = X[collapse[i]].mean(axis=1)\n",
    "        X[f'{name}_std'] = X[collapse[i]].std(axis=1)\n",
    "       \n",
    "    # normalize numerical variables and fill missing values\n",
    "    numerical = norm_paras.columns[norm_paras.columns != target]\n",
    "    for col in numerical:\n",
    "        X[col].fillna(norm_paras[col]['median'], inplace=True)\n",
    "        X[col] = (X[col] - norm_paras[col]['mean']) / norm_paras[col]['std']\n",
    "        X[col] = np.float32(X[col])\n",
    "    \n",
    "    # convert date (to time progress & summer intensity)\n",
    "    dates = pd.to_datetime(X.startdate)\n",
    "    X['time_prog'] = [((d.year - 2014) + d.month)/36 for d in dates] # 2014-2016 are translated to [0,1]\n",
    "    X['summer'] = [summer_weeks.loc[summer_weeks.week == d.week].values[0][1] for d in dates] # 0: winter, 1: summer\n",
    "    \n",
    "    # drop columns\n",
    "    X = X.drop(labels=drops, axis=1).copy()\n",
    "    \n",
    "    # one-hot-encode categorical variables\n",
    "    X = pd.get_dummies(X, columns=['climateregions__climateregion'])\n",
    "\n",
    "    ########################################\n",
    "    \n",
    "    \n",
    "    # change to numpy array\n",
    "    X_final = X.to_numpy()\n",
    "\n",
    "    # info\n",
    "    print(\"PREPROCESSING result:\")\n",
    "    print(f\"  Shape X: {X_final.shape}  || Shape Y: {y.shape if y is not None else None}\")\n",
    "\n",
    "    return X_final, y, X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING result:\n",
      "  Shape X: (300587, 69)  || Shape Y: (300587,)\n"
     ]
    }
   ],
   "source": [
    "# preprocess train.csv\n",
    "X_train, y_train, X_df = preprocess(X_train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING result:\n",
      "  Shape X: (75147, 69)  || Shape Y: (75147,)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, _ = preprocess(X_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING result:\n",
      "  Shape X: (31354, 69)  || Shape Y: None\n"
     ]
    }
   ],
   "source": [
    "# preprocess test.csv\n",
    "X_prediction = pd.read_csv('../../data/raw/test_data.csv.zip')\n",
    "X_prediction, _, _ = preprocess(X_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "# Save dataset \n",
    "--version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare folder and info file for dataset versions\n",
    "\n",
    "timestamp = time.strftime(\"on %Y_%m_%d at %H_%M\")\n",
    "\n",
    "overview = pd.read_csv('../../data/preprocessed/dataset_overview.csv')\n",
    "\n",
    "if not os.path.exists(f'../../data/preprocessed/{version_ID}'):\n",
    "        os.mkdir(f'../../data/preprocessed/{version_ID}')\n",
    "\n",
    "#append to overview logs\n",
    "overview = overview.append({'version_ID': version_ID, 'version_note': version_note, 'timestamp': timestamp}, \n",
    "                                ignore_index=True)\n",
    "overview = overview[{'version_ID', 'version_note', 'timestamp'}]\n",
    "overview.drop_duplicates(inplace=True)\n",
    "overview.to_csv('../../data/preprocessed/dataset_overview.csv')\n",
    "\n",
    "# save preprocessed data\n",
    "np.save(f'../../data/preprocessed/{version_ID}/X_train.npy', X_train)\n",
    "np.save(f'../../data/preprocessed/{version_ID}/X_test.npy', X_test)\n",
    "np.save(f'../../data/preprocessed/{version_ID}/y_train.npy', y_train)\n",
    "np.save(f'../../data/preprocessed/{version_ID}/y_test.npy', y_test)\n",
    "np.save(f'../../data/preprocessed/{version_ID}/X_predict.npy', X_prediction)\n",
    "X_df.describe().to_csv(f'../../data/preprocessed/{version_ID}/columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>version_ID</th>\n",
       "      <th>version_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>on 2023_01_06 at 13_15</td>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>on 2023_01_06 at 13_35</td>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>on 2023_01_06 at 15_27</td>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>on 2023_01_06 at 15_47</td>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on 2023_01_31 at 16_54</td>\n",
       "      <td>001</td>\n",
       "      <td>_coldrop_datesplit_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               timestamp version_ID         version_note\n",
       "0           0  on 2023_01_06 at 13_15      first                first\n",
       "1           1  on 2023_01_06 at 13_35      first                first\n",
       "2           2  on 2023_01_06 at 15_27      first                first\n",
       "3           3  on 2023_01_06 at 15_47      first                first\n",
       "4           4  on 2023_01_31 at 16_54        001  _coldrop_datesplit_"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see overview of existant datasets\n",
    "\n",
    "overview = pd.read_csv('../../data/preprocessed/dataset_overview.csv')\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# use to create new overview-version and DELETE OLD VERSIONS #############\n",
    "\n",
    "#overview = pd.DataFrame(columns=['version_ID', 'version_note', 'timestamp'])\n",
    "#overview.to_csv('../data/preprocessed/dataset_overview.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6da07facdf973c4dbc8723adb4472dbf7f63b6a0b884708ce567b8eaca67ed69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
